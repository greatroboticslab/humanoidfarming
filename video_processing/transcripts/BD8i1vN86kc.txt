AI for Precision Agriculture and Crop Management 2
https://www.youtube.com/watch?v=BD8i1vN86kc
People & Blogs
 All right, good morning everybody. How are you doing? So my name is David Mulla. I'm the moderator for today's session. And I'm at the University of Minnesota in the Department of Solar, Water, and Climate. I do have a couple of announcements about today's program. Unfortunately, the first two speakers have canceled. So I'm going to be substituting for the first two speakers. And I'll be talking about a couple of different topics. One is irrigation-related, dealing with a new institute that we're developing called the Artificial Intelligence Institute for Climate-Land Interactions. And then secondly, I'll be talking about spatial variability of phosphorus. And then we'll resume with the normal program. The other speakers are here. So I apologize for the disruption. Let me just start by saying that there's a couple of flyers in the back about our institute. It's AI Climate. It's a national AI research institute on climate-land interactions for mitigation, adaptation, trade-offs, and economy. So I'm a deputy director of the department of solar, water, and climate. And so here's what our institute does. We work on artificial intelligence, cutting-edge new advancements like deep neural nets and other knowledge-guided machine learning processes to spur innovation in use-inspired case studies. Irrigation is one of our main focus areas. We're all aware that there's changing patterns of our climate. We're getting more intense climate extremes, more frequent rainfall. You know, just a couple of days ago we had these massive storms rolling through Starkville. And I think we're all awake at night listening to the thunder claps and the heavy rain. Land is, you know, the second part. We're looking at interactions between climate and land, and not only in agriculture but also forestry. Those are big land uses in the United States. We're looking at the interaction, how climate affects farms and forests, and how farms and forests affect climate. And we're interested in mitigation and adaptation, which means, you know, reducing and absorbing carbon dioxide and reducing the emissions of greenhouse gases, enhancing resilience, and looking for trade-offs, you know, between the decisions that people have to make surrounding agriculture and forestry. Should I increase my production? Should I, you know, optimize my irrigation? Should I plant cover crops? Should I, you know, go to a completely different crop rotation? These are all trade-offs that have economic implications. And so we're looking at those kinds of trade-offs. So our vision is to transform the science of AI and decision support tools for climate smart practices and to create new tools that solve problems that haven't been solved before. So we, you know, communicate with users and integrate their needs into our approach. The structure of our institute is built on research, education, workforce development, collaboration, and knowledge sharing, and then management and integration. And what we've heard from stakeholders is they are looking for models and tools that can help them evaluate the trade-offs and, you know, make their farming and their forestry practices more sustainable. To do that, they need data. And data is one thing that we really focus on. We're developing benchmark data sets for the United States and the whole world that can be used to train and test AI models. We're also involved in developing educational modules to help people in graduate and undergraduate education in the professional realms better understand how AI works, what it can do, what it can't do, and, you know, become competent in terms of using and applying and developing new AI tools. So we have these strong internal partnerships and team building. We integrate research, education, and extension. We are the lead institution at the University of Minnesota, but we partner with Colorado State University, Cornell University, North Carolina State University, Purdue University, Delaware State University, and Fairview-Van Amity University. So we've got some deep networks that exist. Our research strategy is to use a co-design process where we conduct cutting-edge, use-inspired, and foundational AI research. We use a virtuous cycle to go between the domain science and the computer science. Here's the real details of everything we're doing in our institute. The left-hand column is our use cases. So these are practical issues that, you know, we need to solve, like better models for estimating greenhouse gas emissions, solar gas emissions, solar gas emissions, carbon dioxide removal, addressing climate risks, adaptation, and shifts in cropland forest transitions, and looking at, in particular, irrigation. Multi-criteria, multi-objective decision-making to evaluate trade-offs. Earth economy modeling so that we can put economy and ecosystem services on the same page, and do that at the scale of the whole world. And in multi-scale, multi-criteria decision support tools, digital twins, and things of that nature for soil moisture, for example. We just published a AI paper in the Computer and AI Institute Congress, which is being held in Santa Clara next month. And it's on estimation of soil moisture content for the entire U.S. It's a very novel approach that combines AI tools like knowledge-guided machine learning with benchmark data sets from actual measurement sites at thousands of locations across the U.S. And we incorporate climatic data and remote sensing data from satellites to do the modeling. Some of the knowledge gaps that exist are out-of-sample predictions because of the variability and the sparse data that, you know, exists in agriculture. Hard constraints need to be included in AI models because models can't just be used as black box tools where you don't have any explainability. So we're using mass and energy balance equations to constrain our models. We're involved in computer vision, perception, and analysis to actually identify objects and train our remote sensing models based on ground truth data from UADs or cameras. Then multi-objective, multi-decision, multi-criteria decision-making, which involves a trade-off between economics and ecosystem services. On the computer science side, the last column deals with, you know, the sort of foundational AI that we're using in our institute. Using knowledge-guided machine learning. We're using combining learning and AI reasoning to do better optimization. We're involved in computer vision-guided perception analysis with, you know, tools like YOLO and Segment Anything and other tools as well beyond that, few shots and one shots. We're doing, you know, multi-criteria, multi-objective decision-making with advanced AI tools. And then we're developing digital twins. For example, of soil moisture to help us develop more resilient systems. One of our sub-areas is climate risks and adaptation. And we have five focus areas here. So the one that this crowd might be most interested in is the crop hydroclimatic risks. So we're trying to predict future crop yields under climate change by looking at the interactions of CO2, soil moisture and temperature and temperature. And also simulating what might happen in terms of need for, you know, future irrigation across the U.S. to basically adapt our agricultural systems in the face of warming temperatures and changing climatic patterns. So I think that's something that maybe a lot of you would be very interested in. To summarize, we face a lot of challenges in agriculture and forestry related to, you know, the extreme climate and the events that are, you know, changing every day. Land stewards need better guidance on adapting practices in ag and forestry. Where should they put regenerative ag or climate smart practices? Where should they use them? Which ones should they use? How long should they be used? And then, you know, how can we help make those decisions and evaluate those trade-offs using AI by improving our benchmark data sets for training support tools for developing more accurate predictions and for scaling our algorithms, you know, from the farms to the regions and from the regions to the farms. So we have to go both ways, upscaling and downscaling in a lot of the work that we're doing. I think this is going to benefit AI in the long run because we're learning, you know, how to incorporate things like mass balance into the AI tools that exist. We're cutting, you know, some new ground in terms of AI tools and that includes multi-criteria, multi-objective decision making and trade-offs. So that's the summary of what we're doing in AI climate. There are some handouts on the back seats which you can take. They're free. And they're reading more about our, yeah, go ahead. Just, if anybody needs them, they're on the back row there. So I'll stop here and see if there's any questions. Thank you very much. Yes, hi. Zeke Kermals from Virginia working for the DOD but here as an independent researcher. My question as I see AI climate, very, very important part of the discussion and throughout this whole conference we've been hearing about setting those standards. My question to you would be where do you see AI climate participation at the table in helping us see where artificial intelligence standards and AI climate interact and what standards would be set? All right. So the question was about AI tools and then the standards that are being developed along with those. And you find standards, you mean like ISO standards? So what do you see from AI climate's perspective of what's important? Right. So I know Alex who's here in the room knows a lot more about ISO than I do and he's just issued a major report which was rolled out yesterday from TASC where they do a lot of discussion about ISO standards and the need to, you know, have uniform protocols for data collection and management and interfacing different kinds of different kinds of different kinds of farm equipment products with other farm products. And I think that's an important area for further advances. If, if Alex wants to say a few more words about that, I'll, you know, really, I think he's more of an expert in that than I am. Yeah, no, a whole lot more to add, I guess. I think there were two standards issues that were covered in the CAS paper. One of those is around data standards so that AI is more usable, if you will, for the data that are being collected in agriculture. And I see a group like Ag Gateway as being really central to that effort. Ag Gateway, if you're not familiar with them, are trying to develop data standards, data collection, data format, metadata, all of those kinds of standards for the use of agricultural data, not only in farming but across the agricultural supply chain. Also, there are ISO standards and other standards organizations like ANSI, IDC, ASAPE, who are beginning to look at standards for artificial intelligence. And I think this relates to, you know, so-called trustworthy AI or explainable AI or the functionality. How can, if we're using an AI system, for example, in an autonomous machine, how can we be certain that this AI will perform its function as expected and not cause harm or just fail? So, those are, hopefully, that helps answer that question. Thank you. Thank you. Thank you. And I think the issue of benchmark data sets is also really important here. There have to be really standardized protocols for collecting data. AI, you know, needs rigorous ground truth data in order to train and test models. And, and, and it, and it, not every data set exists, you know, that, that, not every data set can be actually used to train and test AI models. There's some data sets that really just don't have what it takes to do the right kind of training and testing. Any, any, any other comments or questions? All right. I think we'll move on to the second talk here today. And, um, as I mentioned to you, um, unfortunately, the first two speakers did not show up for this session. So, uh, I was asked to step in and, uh, provide some information that, um, could be, um, possibly of interest to you. So, the second talk for this morning's session, uh, is not about mapping, um, you know, uh, the economics, I think it was an economics talk. Actually, I'm going to talk about machine learning, modeling, and mapping of soil available phosphorus. Um, so, in precision agriculture, we need to enhance the profitability and the sustainability of our systems. Detailed information about soil fertility can be used to delineate, um, management zones, uh, where phosphorus is the limiting factor. And, uh, what we want to do is avoid under and over fertilization because that over fertilization helped, you know, produce the environment. Under fertilization, um, you know, reduces crop yield. So, what we did in this project was we tried a number of different machine learning models. Um, we used, uh, ordinary creaking, which, um, you know, was an older technology developed for predicting information at unsampled locations using semi-variograms and, uh, a linear weighted sums of observed values. This particular method does not use auxiliary variables like remote sensing. Um, generally, it's based on soil sampling. Uh, we use support vector machines where we're trying to use either regression or classification to identify, you know, information at unsampled locations. Bayesian additive regression trees is the third method that we use, which is the more, uh, sort of probabilistic method that uses the posterior probability of sol phosphorous to make predictions. And both SVM and Bayesian additive regression trees can use auxiliary data like, um, you know, satellite imagery or, uh, proximal sensor information. So, the objectives were to assess spatial variability at a regional scale using a fusion of different sources of geospatial information, uh, with machine learning algorithms for, uh, commercial farm fields in Ontario, Canada. Uh, so we wanted to compare the performance of the BART, the support vector machine, and the ordinary pre-team methods. We wanted to apply the trained and calibrated models to predict, uh, sol P at a three meter spatial resolution using, uh, the most highly correlated, uh, covariates we could find. We wanted to compare the accuracy of the machine learning techniques with the geostatistical approach and then derive, uh, recommendations for, uh, where farmers should spread phosphorus and how much rate of phosphorus should be applied. And then finally to evaluate the economic implications. So, the area we studied is in, um, Ontario, Canada. We've got an Eastern and a Western zone. Um, the Eastern zone has four fields and the Western zone has three fields. Um, there, the yellow dots on this, uh, figure show the locations where we collected phosphorus samples by manual labor. Uh, at about a two, uh, one, about two samples per hectare. So, it's pretty intensive, uh, soil sampling. Uh, but then we also brought in auxiliary variables from remote sensing and proximal sensing to, um, you know, complement the soil sampling data. We fused the two sources of information, uh, to do spatial predictive modeling with machine learning and geostatistics. And then we got our outputs. So, the flow of, uh, you know, work is, is shown here. It's kind of detailed. Uh, we had, uh, topographic attributes from, uh, three meter digital elevation models. We had proximal soul sensing of electrical conductivity. Uh, we had, uh, planoscope satellite data at three meter resolution. Uh, then we had our measured data at two samples per hectare. And, um, so we, we fused those data and we went through a process for training and testing our, our models against, you know, the different scales. The infield scales, the regional scales, and then the sort of, uh, global scales. Uh, and, um, then we evaluated the accuracy. So, here are some examples of some of the auxiliary data that we, uh, used. There are actually 42 different, uh, variables. I'm just showing you eight of them. Elevation was collected at 3,000 samples per hectare. Uh, this chlorophyll green index from planet was three meter pixels. Uh, the ECA or HCPE one was, uh, zero to 1.6 meters, 2,885 points per hectare. Uh, at a deeper depth, we had the same information for two, the same density. Uh, we have, uh, red and, uh, NDVI in the green. Um, which were then, uh, you know, obtained from planet. And in our ground truth data were the sole samples for pH and phosphorus, which were two samples per hectare. Um, so the descriptive statistics of our, our phosphorus, uh, the mean was 16, uh, parts per million in the eastern zone and 24 parts per million in the western zone. Uh, according to the, um, Ontario, um, Department of Agriculture, any soils that have less than about 31 parts per million of phosphorus should respond to phosphorus fertilizer. So on average you would say, okay, let's just, you know, let's fertilize all these fields with phosphorus based on this average value. Well, as I'll show you, that's not a really wise decision because there's a lot of spatial variability. So let me get to the technical details of the, uh, the BART model. A BART model, um, started out with all 42 variables and then we eliminated the ones that were least important. We arrived at six factors in Eastern Ontario. Those are the ones shown on the upper bar graph and four factors for Western Ontario. And you can see that the, the names of those variables. Elevation was the most important predictor, uh, for both zones. And then we had some infrared, uh, some, some satellite imagery data that was important at both sites. And ECA data was more important in the Eastern zone. Um, so the variables differ somewhat from each, uh, region. But elevation and remote sensing are very important. We look at the performance of our machine learning models. Um, uh, you can see that the BART had the lowest RMSEs, um, compared to the support vector machine. And, uh, you know, the highest R squared values. So it, it performs better. It, uh, doesn't need as many auxiliary variables in order to do the prediction. With the SVM, we, we found that, you know, it, it liked to bring in a lot of auxiliary variables. Like 15 variables for the Eastern zone and 12 variables for the Western zone. That's a lot of auxiliary variables. Um, we, we then said, okay, what if we just took the variables that the BART model said were most important and trained the SVM model with that. And that's what the SVM six star is for the Eastern zone. And that still didn't perform as well as the BART model. So BART model was the best performing model. Here are some examples, uh, spatial predictions for available phosphorus, um, using SVM on the left, uh, BART in the middle, and then the ordinary creeping on the right, which doesn't use any auxiliary variables. So you can see right away using those auxiliary variables like remote sensing and electrical conductivity helps a lot to give us better predictions of spatial variability. And the patterns really look very different if you don't use auxiliary variables. Um, we also can calculate uncertainty with, um, these models. So here are uncertainty maps with SVM on the left, BARC in the middle and ordinary creeping on the right. And from the scale, you can see that the uncertainty of predictions is lowest with the BARC method. We're down at, you know, two parts per million or three parts per million. Whereas with ordinary creeping, it could be as much as 25 parts per million for our uncertainty, which is huge. So, um, all right, so then, then next step was to actually do the mapping and decide on what strategy for management should actually work out. And so on the, and we had two different findings. One was there's a set of fields where variable rate nitrogen was really the best option. And there was another set of fields where uniform application of nitrogen at low rates was the best option. So the top column, top row here shows an example for the field KM, which, uh, required a variable rate application strategy. So you had some areas getting 20 kilograms of phosphate, some 50 and some 70, which is the conventional rate that the farmers are using 70. So on the bottom, you see another set of fields where we were recommending a uniform rate. And in this case, it happens to be about 20 kilograms. So it's a very low rate compared to the conventional rate of seven. Now here are all five fields that require that uniform low rate. Uh, there are five fields here. And on average, we're saving 50 kilograms per hectare of nitrogen for all five of these fields by using our methodology. That's a huge saving. Here are the two fields where we recommended a variable rate strategy. So spatially varying the rate of fertilizer from zero all the way up to 70 kilograms per hectare. And, um, uh, the, the, uh, LN field, uh, requires very little, you know, variable rate nitrogen, uh, fertilizer. It's only about 20 kilograms and the rest requires no fertilizer whatsoever. So there's great savings on that field, the one on the left. The other field has a higher requirement. The large area that requires about 50 kilograms of phosphorus per hectare. And the other quarter requires about 20. So these have cost implications and reducing the PE rates using our approach resulted in a profit of about 15 to 16, all the way up to $72 per hectare, including the costs of doing the variable rate application and doing, and collecting the data. So this is really a big factor for farmers that, you know, they can make a lot of money by changing to these practices. The greatest profit occurred in the uniformly fertilized fields at a low rate, which saved 50 kilograms per hectare of phosphorus. And then you had more, uh, moderate, you know, profits in the other two fields where, uh, we're recommending variable rate. So our conclusions here, there's big economic savings from doing this. Uh, the BART method is, uh, the most accurate of the three methods we tested. And, um, uh, this work has been published. Uh, uh, you can see the first, uh, you know, reference here if you have any questions. Thank you very much. If you have any questions, I'm happy to answer those. Yes, good one. Well, I have, I probably have two questions. The number one is, uh, you know, for owner creating, you can use a code creating process of librarian auxiliary variables to boost up your performance model. So I'm wondering if you can apply that, those variables that you selected from bar and then use a code creating approach to maintain your model. Yes. Yeah, that's a really good suggestion. We didn't try that, but we thought about it. It's just that we ran out of time for doing all these different models. Uh, but I think that would be another approach that could be very, uh, helpful in terms of improving predictions. Yeah, so the, the, the other one I probably, I didn't follow is, uh, when you do a, a bar, a variable selection, what's your criteria for you to decide, okay, these are the good variables that you can do on. Yeah, the, the criteria are kind of complicated. Um, let's see if I can go down here to the bottom and show you another slide on that. I don't know how this works. Trying to advance. Anyway, um, basically what we're doing in our, you know, criteria is trying to, um, optimize the, um, the model, um, to the, um, the model, um, to the, um, to the, um, to the model, um, to the model, um, to the model. Um, to the model by reducing the RMSE. And so, uh, we're looking at, uh, the variable influence that different parameters have on each other and the cross correlation. And removing parameters that are highly cross correlated to get to the ones that are most important. Um, all right, so now we're going to move back into the normal program and our first, uh, normal presenter. Sorry, I was substituting for some others. It's, uh, Chi Yu Yu from the University of Florida. And, uh, let me close this and open your, your presentation. Okay, um, so hi everyone. Hi everyone, my name is Chi Yu Yu and I'm a student from University of Florida. And my supervisor is Dr. Yanis Antides. And today my topic is our lab's latest product. It's our AI-1 precision square with a canopy specific parameters optimization for the enhanced field efficiency. Okay, um, so here's the introductions like why we are making this smart square. And so our target is to, um, develop, uh, AI-enabled smart spraying systems. And we aim to optimize the pesticide and nutrition, uh, application speed of field. And also we want to achieve our goals mainly by, uh, dynamic, uh, automatically adjust spray, uh, spray speed, the flow rate, and the coverage area based on the real-time measurements of our tree canopy density density and the height. And so this is very important because it can reduce the off-target spraying. And also it can help us to reduce our peasantry loss. And also help us to improve the spraying efficiency and enhance, like, is very good for our environment. Okay, so let me introduce our, uh, key components and the whole RT-tecture. So first we use the DAT camera, which is from the Intel RealSense to capture the images of the field. And then we have this field camp data. And then we use the data we use these videos and the input as, uh, as an input to our NVIDIA Jensen Xper, which is already, um, built with the Eccorsense AI, which I introduced yesterday from our poster. And our briefly introduced the Eccorsense. This is, uh, uh, system that can automatically analyze the tree canopy density, the tree height, and it can count the tree numbers, and finally do the fruit count. So, uh, after this we can get our tree canopy density and height estimation. And then we use this and send some comments to our microprocessor, microprocessor ESP32, and, which is, uh, real, uh, free real-time output system. And we use this as a slave controller. And then it sends an email to our DC monitor valve, and, which can help us to control the, uh, liquid flow. And we send comments to the nozzles so it can spray. And also we have this, um, signal dumper, and which can use to adjust the fan inlet size, like to control, uh, the air input. Okay, then it's like, uh, overview of how our spray really looks like. So you can see at the front we have this RGB, uh, depth, RGB depth cameras, and from the captured images. And below we have our control box, which has all of the, our microprocessor and, uh, all the other, like, GPU, IF, uh, GPS, IFU inside. We make it into a control box. And also we have, uh, each side we have power box, which can supply, uh, as, like, battery supply for our processor and our cameras. And finally, at last we have these nozzle cables, which connect to our nozzles. And also, at the back of our tractor, we have this real control dumper, which can control the air input and output. Okay, so come to our, uh, uh, experiment. So we developed, uh, we divided it into two parts. So the first one is the pre-test. So we test it in a test field to, like, try to figure out what the, uh, nozzle sphere range, so it can cover what the tree can be. And also we evaluate the coverage across the canopy zones. It's like, you can see here, we have several zones, and we test how our nozzle spreading works. And also here is, like, in the field, it's, hmm, it's like, uh, uh, a framework, like that. So you can see, we, our tractor move in the middle of the tree rows, and it can spread for both two sides. And the second part is our field test. We test mainly on three canopy desk levels. We have this small, medium, and high. Uh, and also we, uh, test like five citrus trees per level. And also how to measure it, we use the water-sensitive paper, and the front needle and the back of each canopy, so we can see how the sprayer really works. And also we verify the fan-like size and valve setting, so we can, we want to figure out what's the best for the sprayer to get its best efficiency. Okay, finally comes to the result. It's like, uh, during our experiments, we find out it's like 80, uh, 28% increase compared with regular sprayer. And also it can efficiently reduce the spray drift, and also it can reduce the off-target spray loss. And also, because we also have the GPS and IMU, so we can, uh, automatically adjust our parameters to make it work very smoothly. So, and also for our, uh, system, I think the bad thing is quite, uh, eco-friendly because like, you only spray where you want to spray. Like the tree trunk, and you don't want to spray, spray it into the weeds, like when you're doing the nutrition, you don't want, uh, give extra to the weeds. You really want to give it to the trees. And also it's quite good for the farm management, because like, for, for this, it can help you to, uh, more customize, like, which tree you want to spray, uh, specifically fertilizer or pesticide, you can customize it. Um, okay. And here is our, uh, user interface, this player. It's like, from here you can see, like, uh, the altitude and latitude, like, the location of your tractor, so you can know where it's actually moving. And also you have the, uh, you can control the GPS, the cameras, and, uh, so you can, uh, know everything about how your tractor is working. And here is also like, the zones, the settings, so we can know, um, like, which area of your field is actually being sprayed. Um, so, and also on the right side is like the settings of your users interface, with the control inputs for our screen system. So you can, here and here is like, you can set up the distance between the sensors and the walls, and also you can set up the screen buffer, because it's, uh, across-taped area where you want to spray. And also here is like, you can apply it for a specific tree, or you can apply it for all trees in the field, so it is totally cosmetic. See, so, yeah, it's quite, uh, and I think it's quite easy for end users to use, and it's like, you are a technical specialist, or you are just a farmer, so it's quite easy, uh, and to understand and to operate. Okay, um, yeah, finally, I want to introduce our result. It's like, you can see from here, it's a heat map of our spring, and our sensors, uh, view. And you can see that we generate our, uh, heat map, and you can see, like, exactly how many volume you spray for each tree. Uh, and also here presents some speed of your tractors, and the flow, and the total gallons, and also, because our aggregates also have the fruit count series, also fruit numbers showing here. So, um, yeah. I think that's also any questions or comments? When you were coding UI, did y'all look at, um, what language y'all were going to use, and what, how did you go about developing the UX for the system? Uh, uh, uh, actually it's like, the basic website development, like Java, and also for the Outlook, we used the JavaScript, like, to develop everything. And it's actually linked to a database containing all the information that, um, we know about the field. It's like, everything will start in a database first, and we present it as a software interface to our users. Anything else? No? Okay, I think. But we have more time than this. Yeah. A couple minutes, so. When you were looking at the engineering side of this, and you were building the sensors, did y'all look at what type of sensors you wanted to use? Because I saw you use LIDAR, and then computer vision. Yeah. Um, yes, but everything really comes from the DAPT camera. It's like, it will come up with the RGB image, and also the DAPT image. And we, our, uh, software interface comes from the error sense. And so, uh, and it's based on, like, machine learning stuff. So, we use these pairs of RGB and DAPT images as inputs to our machine learning models, and come out with the result. And also for the LIDARs, um, if you want to more precise results, or if you want a cloud, um, cloud map like that, so you can use the LIDARs here. And, I think the result is quite good, quite precise. Thank you. I'm curious about the marketability of this. Do you have intention to take this to market in some way? Um, yes. Yeah, we want to make it, like, commercial. Um, and maybe we're thinking about, we can, uh, sell our product, because everything we can make is portable. And, but for the, and for the DAPT, we can help send some technical specialists to the farmers, and we can build everything for them. And we can do the test product, maybe, and see the result. Help them to learn how to use it. Yes. All right, thank you very much. Thank you. Thank you. Uh, so our next speaker is, uh, Jing-Gi Chen from Florida A&M University, and she's going to talk about, um, growing precision, let's see here, growing precision engineering systems for grapes. Okay, thank you very much. Thank you. And I'll give you a more in the next turn. Okay, that's up. All right, good morning everyone. How are you doing? Good morning. Good, good. So, my name is Jing-Gi Chen. I am a system professor with Biological Systems Engineering in the College of Agriculture and Pro Sciences at Florida Agricultural and Mechanical University. And today it is great to have this opportunity to talk to you about this exciting work, which is growing a precision engineering system for muscatine grape vanguards to aid the climate-smart agriculture. So, let me start with a little bit of background and motivation. So, muscatine grape, they are native to the southeast U.S. and they survive in the humid and warm environments where other varieties of grape typically struggle. But water management is very crucial for muscatine vineyards as well for several reasons, including irrigation needs, disease prevention, soil health, water use optimization, and fruit-to-quality improvements. So, to monitor the soil, the vine, and the environmental conditions are essential for efficient resources used and vineyard management. And then, let me introduce you a little bit of background about our resources. So, Florida A&M University is a HBCU and we are the number one public HBCU. And we have this umbrella on the College of Agriculture and Food Sciences. We have Center for Viticultural and Small Food Research. And this is, this center is recognized internationally for the warm climate grape research. And the center maintains the most extensive muscatine grape geoplasma collection in the world. And this is one of the five national clean plant centers for grape. So, we do have a great resource. But looking at some of the gaps here, we are lacking some information regarding muscatine irrigation. So, we all, when we concept the project, we want to build this precision engineering system that can do some of the near real-time monocry in the vineyard to provide some of the decision-making information regarding water management and disease management. So, we piloted this project using nine muscatine grape vines with three varieties. One variety, two variety, one variety including Norvo and Floriana. One white, one variety, A27. So, we want to deploy the Internet of Things sensors and network communication. And then we want to build the precision engineering system. And we also want to facilitate the student explorational learning, precision ag and digital technologies. Because we are academic programs and we do education. For me as a citizen professor, I have a relatively high teaching load. And we also want to increase the community engagement through extension and outreach. So, let me show you a little bit of foundation of this work. We finished an AG2TI SIG grant in 2022. Based on that outcome, we were able to build an online portal here within the Peer Learning Agricultural Network, the plan. This is an in-house developed website. And it can capture the different data types from the data generated in the farm. For example, like when to spray, when to harvest, when to apply some fertilizer. And the house, we can take some images and then store it in this portal. And this is like a user secure. We can create an account for each, for phone manager, each user. Like a student or researchers. And then the portal also can gather data that is monitored by the IoT time or tree sensors. And this is, here, this is a 55 acres of research bay yard located in the east side of Tallahassee, Florida. And it's, so we basically created this portal, this portal to store those information. And this is a customizable, we can create a base on the user's needs. For example, if we demonstrate the portal to farmers to growers. And then these are some features we can introduce. And then through the project, last year we were able to deploy the internet of the sensors and network. We did this in February 2024, early growing season for muscadine grape. And we have installed nine setups of canopy cameras and sensor arrays, including the soil moisture probe, ring gauges, relative humidity sensor, temperature sensor, and leaf wetness sensors, and also weather station. And this is our in-house designed, I'm sure if I have a point here, maybe it doesn't work for this screen. But the upper right corner, that is our in-house designed canopy cam with the edge computing nod. Let me show you here in a minute. And then this is how it looks in the field. So above the canopy, we have the raspberry pi powered canopy cam. above the canopy and it's powered by the solar. We don't need any actual power in the field. And then this is our weather station. And underneath the ground, we have those different sensor arrays, like buried underneath the ground. So we implemented the edge computing sensor nod. This is the camera that can capture and process in the canopy, cover images on site, and then transfer back to our server, which is that website that I showed you earlier. And we developed the image analysis of algorithms using Mahalad Nobic's algorithm for automatic data collection and analysis that enabling the estimation of penalty cover and grade percentage that can show using the website. Alright, so we use the portal for data aggregation and visualization. So the near real-time data can be accessed by researchers, students, and growers. And we integrated the LoRaWAN technology for our low range, low power data transmission from the field sensor back to our central database. Also the weather station data, also we can use the API to transfer back to our central database. And let me show you some of the results in the field. These are one of the demonstrations from a one of nine binds we monitored last season. The canopy node installed above that can capture the canopy images every 20 minutes from 7 a.m. to 7 p.m. Because of the power, we want to save some power from solar during the night. So we just program the canopy can to sleep. Okay, you go to sleep during the night and then 7 a.m. Hey, wake up and then do your job. Take the images every 20 minutes. Okay, and then the raw images will be stored on site. And then the machine learning algorithm will precise the images on site. And then transfer back the canopy cover and rate percentage to the central network. So the LoRaWAN is the transmission profile which will work with narrow bandwidth with low, basically, payload size data. So instead of transfer back for megabed image data, we also transfer the precise the cattle size data. Alright, and this is an example to show how it changes for that canopy cover along like during the season. This is a captured for last season April to June. And we do have like the whole season, last season, and this year we start a new season. And here is an example for the near real-time soil volumetric water content that collected in one of the locations. We do three depths, zero to one foot, one to two feet, and two to three feet. So we have the sensor averaging four inches, but we aggregated the data to show three depths. Because of the muscadine grain is a shallow root creature, so you can see the water use variation will show in the topsoil, typically zero to 15 centimeters. Alright, and here the higher number indicates the wetter soil, and the lower number indicates the drier soil. And this is another example for the near real-time soil temperature that collected using one of the locations. And same, three depths, zero to one foot, one to two feet, two to three feet. And again, the topsoil shows the greatest variation for the soil temperature. And here is a demonstration of the near real-time ambient relative humidity and temperature that collected for our wide-aligned variety, Floriana. And this is near real-time precipitation and leaf wetness from last season. And the weather station helps us to manage and monitor and tell parameters for the ambient environment. So, near the vine, we have the metal station. So, if we have some, for example, missing data or some of the growth of the vine during the season, you know, the leaves kind of like cover those macrosistation, we use the weather station kind of to help with missing data. Alright, so this year we just initiated, we installed our upgraded canopy cam in early March. And this year, because the winter, we have a colder winter compared to last year. And you can see the body break stage is late this year. So, we initiated the feature for grapevine body detection because the body break images is very critical for musculine grape. They need the water during that time. And for developing the body detection feature, we used the deep learning algorithm, ULO. And we used the images collected in the field last year as our training dataset to train the model. And we have some of the validation dataset separated from the training dataset. And this year, we are going to collect more as a field ground truth to test the algorithm. Alright, so through this project, we were able to initiate a digital app and plant phenotyping technology experiential learning opportunity. We were able to provide a semester-based internship and also summer internship for our students, including graduate students, undergraduate students, and community college students. So through this, it provides a strong support for HBCU students and also add the digital app components to our curriculum. And also help with the recruitment from a community college and then transfer to our university to finish their bachelor degree. Here's more pictures about they got a good learning opportunity. And we also use this project, this platform to provide community engagement through extension outreach. Some of the platform including annual Florida A&M University grape harvest festival, we typically get about like 2,000 participants so we can showcase how does it look like for this project. We also engage with other communities, for example, in Albany, Georgia, some international connection with Kenya, Argentina University faculty, they also get some training from us. So to summarize, during this past year, the first year of this project, we built the very first precision engineering system for near real-time monitoring for musculine research, the Banyard at Florida A&M. We want to take this as a test bed to show some of the advanced engineering technology and how to use them, especially for stakeholders, the growers, researchers, students, how they can use the data to do more analysis. And we were able to build the infrastructure and platform for students to use to learn digital ag and plant phenotyping technologies through the project. And one thing I want to mention about this environmental resilience, last year we do have a lot of challenges regarding the extreme events, okay. In Tallahassee, Florida, well, the nine setups, they survived from several heavy thunderstorms in May, April and May, and we had one tornado in May 10th. And we have two hurricanes, Hurricane Zaggy and Hurricane Halene. So very unfortunately, very fortunately, our setup, they survived. They still functioning very well. And the system helped the farm manager to detect some of the deficiency of the irrigation emitters. So we were able to replace those things to help the farm manager. All right, so with that, that concludes my presentation. I want to thank our funding agency, National Science Foundation, to provide us funding for the students to do experiential learning regarding digital agriculture and plant phenotyping technologies. And also thank Yusin and Nifang for all their support for research, extension, and education. All right, thank you. I think we have time for only one question. So, um, anybody have a question? As a result of all the challenges of last year, how was the harvest? The harvest, um, it's, um, so last year, because the latest season, we have a very wet season. So you will see from our monitored canopy images, you will see a lot of disease. Uh, for example, the angular leaf spot and black rot developed in the late, uh, in the season. Even if we do regular screening, um, but a lot of wash off because of the, uh, precipitation. But regarding the harvest, it was, uh, compared to the year before, not that bad. We still be able to get, uh, uh, average day, like, a hundred, close to a hundred to, between a hundred to a hundred fifty pounds combined. And then we're using those spots to making wine. It's actually right now at the, uh, fermentation stage. Thank you. Thank you very much. Thank you. All right, we'll move on to the next speaker. Let me get it set up. All right, uh, Samuel, or the drum, or the drum, sorry. Uh, it's gonna talk about agro LLM. Uh, retrieval augmented generation and large language models. Uh, yeah, uh, it's gonna be uh... Good morning everyone. I'm an assistant professor in the state of the university. So I have a background on computer science and engineering, and I used to work with computer vision and cloud damage models recently. So today I'm going to present an apiculture specific LLM called Agro-LLM, which bridges the gap between the theoretical knowledge and the practical applicability for the farmers. So large language models are transforming the agriculture in decision making, precision algorithm, and knowledge sharing. And Agro-LLM is the kind of one which helps to have an agriculture specific LLM which helps in decision making and knowledge sharing for the farmers as well as the education. So here in this LLM, it's more specific about the agriculture information, so we joined the model with an agriculture. So we have the model with an agriculture-related data set, which has the data from the textbooks which are referred in the universities. So this agriculture LLM can be used as an education as well as it can be used for decision making in formal. So LLMs are prone to halcyation, which means like if they are asked with some specific questions related to the core piece of agriculture, then the LLM may come up with some irrelevant responses. So to overcome this challenge, like we have used the RAD, which is the Retrieval Augmented Generation, which helps in retrieving the information which is specific to the corey and it will generate the response with some more relevant answers to the corey. So the other LLM can be used in education because we have used data which are the textbooks which are referred in the university. And it can be used as a teaching assistant as well as it can be used in the forming, used by the farmers for having some decision making regarding the crops. So here we use the structured database where we have taken the three, four important things from the agriculture and then we have taken all the categories, all the sub-feits within the four important fields and we have retrieved the textbooks and we have used those textbooks for, as a database for this LLM. So the methodology I've overviewed, so here it's more of the important thing about the data and then the prompt engineering. So data and prompt engineering makes the LLM stand out compared to the other LLMs that we have in the agriculture field. So the first thing is we have categorized the data in the fourth field. So one is the life science, one is the life science, agriculture licenses and the agriculture management, agriculture forestry and agriculture business. So within the agriculture life sciences we have the subtopics like agriculture policy studies, agriculture law, agriculture communication and emerging research fields. And within the agriculture management we have the subtopics like agriculture business, finances, and then land and resource utilization topics. And in the third we have the agriculture and forestry. Within the agriculture forestry we have topics like crop and soil sciences and agriculture. And we have agriculture and agriculture business category. Within that we have ecological restoration, marine sciences, organic farming and animal science. So these are all the, these four are the four main topics and within that we have the subtopics and we have the sub-topics and we have all the textbooks which are referring to the industry for all the subtopics and we have used that as a data for this LLM. So the framework, the framework consists of a database which is the, so once we have, we return the text and the embeddings from the data which we have and then we are storing that in our database which uses the Facebook AI-signite-search search which is the device which is a vector database which is good in retrieving the information from the database. And then we have used a rat which is a data, almost a generation to retrieve all those information which is stored in the database. And then we are giving that, the data information plus the prompt, the internal prompt to the LLM and then the LLM will be generating the response based on the retrieved information and the prompt. So we have, for evaluation, so we have compared with the LLMs like Gemini, 1.5 Flash, ChatGP4, Go Mini and then Lister. So we have compared the model with three types of LLMs. And here is the workflow of it. So here we have the user query. We have the user query which is given by the user. Once the user query comes in, we will be extracting the embeddings from the user query and then it will be passed to the and we will be sending the user query to the vector database which already has the embeddings from the database. And within the database, we will be searching the screen, we will be doing some similar research and we will be searching the information which is related to the query. We will be matching, it will match and we will find all those information which is related to the query and we will be returning those information. Those information will be passed to the LLM along with some prompts. So here the prompt engineering makes a major role here because if we are doing the right prompt, it will answer the LLM and it will come up with some accurate answers. So we have used the prompt engineering and the prompt plus the required information is given to the LLM and the LLM will come up with a response from that. So we are just narrowing down the search for the LLM so that we can come up with some accurate answers rather than having some hallucinations to get into the answers. So here is a database which has the large amount of information which are the chunks of information. So this is how the WebTor database will be, looks like. So it has the embeddings which are from the database which we have, which we have. For example, here we have the information from the chunks or the chunks of information from the textbooks. And once the query comes in, once the query embeddings pass to the WebTor database, it will search the information based on the query embeddings. And if we try to find all the relevant information from the chunks of information, and the RAG will return, finally the RAG will return all the relevant information which is relating to the particular user code. And then the required information along with the prompt is passed to the LLM for generating the response. So here is the overall integration of the system. So here we have the user. The user will view the query. And the query will be extracting the embeddings from the query using the embedding model. And we have the database which is specific to agriculture. And we have the documents which has the textbooks. And we also have the research articles and few blogs which are from reliable sources. And then those documents. From those documents we will be extracting the text. And then we will be doing that to the embedding models. The embedding models will convert those texts into embeddings. And those embeddings will be saved into the WebTor database which has the great AI-similar search for searching the content within the WebTor database. So now the user gives the query. The query will be passed to the WebTor database and we will be searching for the relevant information based on the query embeddings. And then the data comes into the picture and we will be retrieving the information. And then we are using the retrieved information along with the prompt and pass that to the LLM. And based on the information and the LLM will be coming up with an answer. And in case you, the query, in case if you are not able to find the information within the WebTor database then it's, the query will be passed directly to the LLM. And the LLM will come up with some answers based on what it has, based on what it was trained on. So here are the model evaluations. So we have evaluated the model using the mean, reciprocal, rank, recall, gen, and blue school. So these are all the evaluation metrics we have used to evaluating the model. So we evaluated the model based on the relevancy of the responsiveness for generating for the query. And we have categorized the data into the four main categories of agriculture. So, agriculture, life sciences, management, agriculture quality and business. And finally, we have also the precision algorithms. So we have used 120 questions from the precision algorithm textbook and we have evaluated the model. So, based on the response for generated by the model for the 120 questions, we have come up with some accuracy scores. And the time complexity. So here, the overall, the chart 54-0 mini performance was an accuracy of 93.4 and the time average time for one query is around 10 seconds. So, of course, it's going to be huge. But Gemini, in Gemini, like the performance is big low, it's around 8.4%, but the time complexity is good. It's better than the time to be. So, these are all the things which we come across after the evaluation. So, key findings. So, Chargip, before winning the draft was a better performance for agriculture coding. And, of course, we have used to find a data base for doing the similar research for for research and the development of information. And, the RAD based response to improve the response without hallucination. And, conclusion. So, it bridges the gap between the gap between the theoretical gap and the practical of applicability for the formals. And, also, it can be used as a teaching assistant for agriculture studies. And, it is more specific data. We have the most specific data base for agriculture information. And, the future, like we plan to include the real-time IoT agriculture data. So, that it can come up with some data insights for the agriculture. And, we plan to include also the multi-language support for global forming communities. And, it also, we also plan to enhance the scale for adaptations. Thank you. David, you have the one question. Okay. David, you have the one question. Okay. Great. So, you can see that RAD is giving some good results. Where the models fail, are they failing because RAD has pulled out all the information? Or, is it failing? So, if the question is, if the, if the, if the query, if the query is out of the context which we have in the data base, then the model will fail. In that case, the query will be passed directly to the LLM. And, LLM will come up with some answers with its knowledge. If you are using the chat in the API, chat, it will do the response statement. Thank you. That was really an interesting talk. I'm sure that we could have had a lot of questions on the topic. Thank you. Alright, Ezekiel, you're up next. I'll get your presentation loaded. Thank you. So, Ezekiel McReynolds is going to be our next speaker talking about cybersecurity in AI-driven agriculture. Ezekiel McReynolds is going to be our next speaker talking about cybersecurity in AI-driven agriculture. Ezekiel McReynolds is going to be our next speaker. Good afternoon, or slash, good morning. We're kind of right in the middle of both. My name is Ezekiel McReynolds. I am a data scientist that works at the Department of Defense. My background is I'm traditionally trained as an agricultural engineer and was given an opportunity to apply modeling and simulation through the techniques that I learned from biological simulations into the Department of Defense's questions when it came to modeling and sin. However, here I am an independent researcher, so all of these statements and opinions that I make here are of my own. They are not of the government and they are not of my current employer. So with that, I'm going to step into, that's probably the wrong button. There we go. Step into the next one. So cyber security, what is it and why do we care? The analogy I like to give is you had a group of archaeologists and they plotted out three spots that they were going to dig down to figure out who was the most advanced technological group of our time. They dug down in New York and they found phone cables and they said, wow, they were way ahead of their time. They were able to give a great representation of technology. Then they went out to California and they dug down and this has fiber optics. There's no way there was anyone more advanced than here in California. And then they dug down to the same depth in West Texas and found nothing. And they said, you know what? Nope. West Texas, most advanced civilization ever. They had wireless. That's why I'm here. We are getting a connectivity that's bringing everyone closer. That's a great thing. But with connectivity does come a level of vulnerability that I want to bring an awareness to. The threat landscape of critical infrastructure is expanding and growing at an alarming rate. I'm going to get further into it on a ransomware topic, but there is, it's a multi-billion dollar industry from a hacker side and a cyber security side. So where does agriculture fit in such a largeness? And then the emerging technologies that we're here talking about today, that underpinning is data, connected data, and now we're adding in on top of that another convoluted layer that can be symbolized into AI. So I have a couple of things up here to talk about. Windows XP is now officially overtaking Windows 8. And the question is, hey, why is that? It's solid, it runs, but it does have certain vulnerabilities. We're bringing technologies into areas where Windows 95 still may be running on certain aspects of equipment that we are introducing in Internet of Things. So as we move forward and bring this connectivity, we need to be mindful of what does that mean? Not that we need to get rid of it, not that we need to push it out of the way, but what does that mean? And as I mentioned here, in the critical infrastructure, we have had hospitals ransomware. We have had the Colonial Pipeline, our energy intake in the U.S. shut down for a certain period of time. We need to start thinking about, as we bring in those technologies, what are we going to do about it? So, IT, information technology, OT, operational technology. Precision agriculture is where we meet in the middle now. It's not IoT, that's Internet of Things, very important, but not here. So, what I want to kind of set the playing field when I talk about this is the use of computers and software to store data, information technologies. Really trivializing it, but that's important to understand. Operational technology, supervisory control and data acquisition. We just talk a lot in the industry over there about what we're going to do with ISOBUS. So, that's an operational technology that is getting connectivity and creating data, thus becoming an information technology in the process. So, we need to understand that this is happening and we are the drivers for the agricultural community, the vendors, the research institutions, the extension services. We need to help translate what that means. So, here I am going to talk about the attack surface that is cybersecurity and what that is. So, I went in order of severity. So, least severe to the most severe. They are all severe, but what I see from my vantage point of what could happen as we move forward as a community bringing precision agriculture to the world. Data poisoning. It's a type of attack where you are able to get into the AI model and change and manipulate. This is a very high level thought process. It takes a whole lot of manipulation and understanding of various things. In the cyber world, we kind of think in scales of threat actors. And this is something I would say, again, my opinion, a nation state would be looking at doing. Something from a very critical infrastructure, very silent, very unknown. Then we get into botnets. Now, this is where the rest of the hacker community is going to kind of sit. Because now we are setting up a whole bunch of open internet devices. So, what is a botnet? A botnet can be a whole lot of things. But basically, imagine it as one computer having a whole bunch of spiders telling those spiders where they want them to go. And you can knock other things off the internet. So, I see in the near term, we are going to have a lot of intrusions. It will do a whole lot of damage to the sector itself. But it is going to utilize it as a vector into other sectors. That is where I see one of the very first applications in an IoT standpoint from sensors and other aspects. Now, the most important one that I am here to talk a little bit more about today, which is ransomware. I am sure we have seen aspects of it in our daily life. It is a billion dollar industry now. There is an entire industry set around developing an infiltration, selling that as a product, setting it to other people to then implement. And we know that if it is a critical infrastructure now, it is a place where they can make some money. And so, as we think about this, again, I am just stomping my foot over and over. We are creating great opportunity, but we are also creating a very unique space for actors that we haven't really been considering. Because I came from an agricultural engineering background and I had no idea what happens when it means we enter the internet. So, I have already talked a little bit about it more. I am going to dive in a little bit more. AIML. A Ukrainian general came out and said, oil was the past. The most important resource we have as a society today is our data. We are becoming a data-reliant society. Now, it seems like in the back of your head, no, I know I could be on my own. Now, I can tell you, from my generation backwards now, we lose our phone, we freak out. So, there is a level of data reliance that is being created in the information age that we need to understand. And it is something that is important because in order, as we saw at the very beginning, the population is going to continue to increase. We are pretty much set on how much land we can use to farm. So, precision agriculture is super important. I don't want to be up here and say, we should never go into it. That is not what I am saying. I really want us to go there. But I want us to go there in a very systematic and safe approach. Because if we do not secure our emerging technologies, we don't set up those preparations. What we can do as researchers and bring awareness. Yes, not the cyber security awareness that you click through and you never look at. And then you click on a link and we are back to square one looking at it. This is a discussion showing people what interconnectivity looks like. And so, data is our most important aspect of our day to day lives as we move forward in this AI, ML industry. So, now my suggestions. They are going to seem simple. But it is kind of not forehead slapping obvious. But it is the same things we did when we were running DOS. You didn't trust your computer. You still shouldn't trust your computer. Backups. Data. What happens when the implementation is planned and you need to set that forward. So, we are going to talk about resilience over prevention. You are never going to build a strong enough castle. And that is okay. We are in an open source community. We should be an open source community. So, what does that look like for us as a community? Incident response and recovery plan. As a university and an extension service, I think it is our duty to start looking at the opportunities that we can start introducing. Not only we are saying, hey, here is a way to create data. Here is a way for us to help you set up and secure your data. And lastly, because this is an AI conference and I know I am stomping on data and I am stomping on all this other stuff. There is a very, very unique aspect to AI driven threat detection when it comes to implementing a critical infrastructure. What I mean by that is there is a concept called a zero day. Zero day is a type of hack that no one in the community has ever seen before. We are trying to implement this now in various areas, but it is showing very effective. Because human interaction is the key to how we look at and find those new threats. So, with that, my conclusions is securing AI in agriculture is not just about protecting the data. It is about ensuring global food security. So, at this time, I am going to just open it up, field up for questions. Thank you so much. Thank you. Yes. Do you currently see a need for risk assessments in the agriculture space? Because currently I am part of the OMSI State team that is doing cybersecurity assessments for small businesses. Are we looking at other areas of personal infrastructure? So, how important would you say it is for them to establish a baseline first? And the response plans and other things? Absolutely. I think this is the time to start doing it. I think as cybersecurity professionals should be in there. Because what happens is they finish, just like we are talking about all of the emerging technologies, they finish building it. And then you show up and go, watch me hack this. And they don't want to bring you in anymore. So, setting up those incident response plans, they are going to feel a lot more confident about the technologies that they are bringing to the groups, vendors, industry, wherever we are looking. So, yes. Simple answer. I did have a second question. Oh, okay. So, in a lot of areas in Mississippi where they have a lot of agriculture, you know, obviously you have a rural problem where there is a lack of infrastructure to set up. Some of this infrastructure that you need to monitor some of this stuff. So, do you have any ideas in mind on how to help expedite the establishment of this infrastructure? Because you don't have infrastructure. You can't have a monitor. You can't have a SAR security in place. Yes. So, the, I have been giving this a lot of thought. First and foremost, cybersecurity, if you look at it, also is a multi-billion dollar industry. Very, very expensive in a lot of ways. And there are simple aspects that we can set up from a threat detection monitoring that with the HughesNet network. I don't know if anybody has ever had HughesNet network out in their areas. There is still some simple low detection monitoring that can be set up from an IoT standpoint. Because we're bringing in both, I think there's an opportunity to simply slide in with the data collection that monitor. So, yes, there is. It is going to be a difficult challenge and that's why I want to be here now to have this conversation as it gets brought forward. David, do you have a question? I might have just gotten a message that I was going to ask for people building products, building data products. Is there something we should all definitely start to stick in? I'm going to get slightly technical real quick. The internet is a very much so duct tape and bailing wire industry. If you didn't know, it actually was created by DARPA way back when and then Xerox showed up and IBM showed up and now we run off the ethernet and then we ran into wireless and now we have 5G and now it's really confusing. The backbone of the internet is packed. So, having something that captures simple internet traffic back and forth is very key. You're already actually doing it with the data collection. You're simply throwing out the headers that you're utilizing that are bringing in that data. It is very simple from a deterministic standpoint when you do data collection out in any of the areas to also underpin that to capture the network traffic in a NetFlow manner. Which if you want to learn about NetFlow and IP Fix they're very fun in new ways to capture large data in very small compact forms. Alright, we're going to have to stop now but thank you very much. Thank you very much. Thank you. Alright, I think we're done for our last speaker now. Can you get that loaded? Okay, so our last speaker is Jennifer Johnson and she's going to talk about understanding how emerging ag technologies impact cyber security. So, follow on the same topic. Great, thank you. So again, my name is Jen and just to differentiate between Zeke and myself because we did touch base saying we had very similar topics. Well, Zeke focused more on security. I'm focusing more on privacy. And a lot of people have no idea what that difference is. So, let me tell you real quick. My dad built houses when I was growing up so this is how I think about it. Security, it's your pipes, it's your plumbing, it's your walls. Are your walls square and plumb? Does the water run appropriately through your pipes? If you open and close the door, is there a sensor that says you open and close the door? Certain people have the alarm code to get into the house versus some people do not. Those are your security tools and frameworks. Privacy is your interior designing. It's where you input your TV. Who gets to hold the remote? Because we all know not everyone gets to hold the remote or change the channel. If you have a picture on the wall, that's your data. Who gets to choose that picture? Who gets to choose who comes in to see that picture? Who gets to choose and have authority to move that picture on the wall? That's the difference between privacy and security in a nutshell. I discovered the importance of privacy in agriculture several years ago when I was at an identity conference. So, whoever has a digital identity online. I happened to be walking to next to this gentleman who looked very alone. I struck up a conversation and he was from John Deere. I said, what on earth does John Deere have with identity information? He blew my mind. John Deere has a ton of information about people's identities. They collect them from their equipment. You all know this by now. I fell in love with the topic of privacy and agriculture. I got somewhat distracted by healthcare and security. So, I spent a lot of time being a chief information security officer for healthcare companies. I'm returning back to agriculture. It's fascinating. I'm going to blow through some of this because I know I'm between you guys and lunch and you already know a lot of this stuff. But types of data, there are several different types of data. Almost all of these are going to be collected by the technologies that we've seen presented here today and discussed academically. So, public unclassified data, that's just anything you find in the old school telephone books that's out there. Personal information, any data that can be reasonably inferred to identify who you are. So, your name, date of birth, address, social security number, that kind of thing. Confidential data and proprietary data are more about what's exceptionally sensitive and may be captured as sensitive data in the agricultural industry. So, your confidential data is going to be, your really sensitive might be financial data that your start-ups or your farmers have. Whereas, different proprietary data might be those trade secrets that those different farmers or ranchers are trying to capitalize on that they don't want other competitors to understand. Protected health data, that goes back to healthcare, protected health information. And, consumer health data is like data that's on your Fitbit that's not protected by HIPAA. The Ancestry and Me, or 23andMe, Ancestry, that's all consumer health data, not protected by HIPAA. So, it's important for everyone to understand that depreciation. Let's see, you know this slide, so I'm going to go back to it. You pretty much know this slide, too. So, you have a lot of benefits to data collection. That's why we're doing all this advanced work. What I want to focus on is that there are also risks to this data collection using automated information, artificial information systems to capture that data. But then analyze, change, disclose, sell, use in different ways that you were not intending for that data to be used. There's a lot of different risks. And so, it's worthwhile understanding what those risks are. And those risks are to the individual farmers, ranchers, forestry departments who are contributing that information. It's important for the companies that are providing the technology to understand what those risks are, as well as to understand what that implicates for the greater society. So, you have data misuse, which again, if someone collects your data and uses it for a purpose that you didn't agree to give. Financial loss and competitive disadvantage. If someone comes in and you get struck by ransomware and your product is suddenly, you know, you're just, you've lost your funding, you get slapped with lawsuits, no one's going to trust you in the market again. You have huge financial loss. Sometimes, sometimes breaches are just, they're game changers. They just shutter the whole building. That reputational harm, loss of trust, identity theft is still out there. It still happens. Business continuity, as you touched on, productivity. If you have a business continuity issue and you're completely reliant on automated systems or drones that don't have backups, then you're down. Data prominence and ownership issues. So, I've been, I love reading a privacy policy. I don't know about anyone else. But, I've been going around and reading the privacy policies of a lot of companies that have been showcased by other speakers or that are out here. And, it's interesting how they disclose data ownership of data that they collect and who has rights to that data. Including, if you're going to be longer using that product, some of them say they may delete the information. And, that doesn't mean that they will delete it. That may mean they keep it in perpetuity and do additional things with it. But, you're not using that product anymore. So, is that data your own? So, you need to be aware of those sorts of things. There can be reduced innovation creativity to data collection. I actually tend to not agree with that one too much. This is what I really wanted to talk about. These best practices are also known as the Fair Information Practice Principles. In 1973-74, the Housing, Employment, Education, and Welfare Department of the United States recognized that, oh, these things with computers and databases and there's information. We should probably have some guiding principles on how to protect the data and provide guidelines for federal employees. So, the fifth was created. This is the foundational set of principles for every single privacy framework and set of guidelines in the world. You can take every single privacy framework, GDPR, Pipeta, APEC, they all come back down to the fifths. So, privacy is largely industry agnostic, technology agnostic, and geography agnostic at the principle level. So, it's always good for different industries to understand how these principles can be deployed. And so, again, I wanted to walk through these. It's interesting you can have security without privacy. You cannot have privacy without security. So, that's a side note. I'm not going to go too far into security since you just touched on it. I will say that there is a well-known triad called confidentiality, which means the data is secure. Integrity, which means that the data is correct, and in real time, out-to-date, and availability that the systems are out of the running. But we'll walk through the rest. And so, transparency. And I want to take a moment also, like, how can people who are using these products maybe minimize the power imbalance. If you have a company that's coming in and saying, for example, transparency. I read a privacy policy yesterday about one that was featured in the discussion. And they have their privacy policy for if you interact with your website. And they have it linked several times to the privacy policy of using their products, which is what I want to see. What are they playing ownership of and deletion and what information are they passively or directly collecting? You can't get to it. So, it's a dead link everywhere. They're not being transparent about your privacy rights with their products. And that's a power differential that's towards them, not the farmers. Individual participation. Again, a lot of these websites talk about consent. And that they will only use your data that they collect directly from you if you consent. But if you go into the fine print of those policies, they talk about a lot of data that they indirectly or passively collect, which you do not consent to, which is like your operating system, your geolocation, your, you know, when you're interacting with the system, date, time logs, like what, then what are they doing to keep those? Keep those secure. The authority, who gives them the authority to collect this information? With government, you usually understand it's a government directive, but you have to also understand what your consent is authorizing these organizations to do. Purpose specification and use limitation. This gets to my favorite use case that all product and tech guys like to throw out there. Some future use case. We're going to keep your data indefinitely for some future use case, which I, as a consumer or farmer or rancher, have no understanding about what that is. And not consenting to that. It just gives them all rights to your data to manipulate it, change it, sell it, if they keep it for some future use case. So you always want to make sure that in your contracts, the wording is very specific as much as you can get to it. Minimization, there's a principle that says you give the least amount of data that you need to do in order to perform the service or good that you're seeking that product to deliver. So if you're seeking information about plant health, it's really odd to me, for example, there's a crop platform, a crop health platform I was looking at last night and this morning. And then I went to their privacy policies and then I looked at the platform for their privacy policy. And it's all about crop health. That's it. But in their policy, they describe that they are collecting and storing and using your credit card information. What is, why do they need credit card information? Also nowhere does it say that company is PCI certified. So they have a lot of security goals in that. Quality integrity. What happens if you have an AI system, analyze your data, it gets published or released, but it's wrong. How do you have, what's your redress rates to get your data updated? Access amendments, very similar to quality integrity. Secretary, we talked about an accountability. What is the accountability for organizations to remediate or change if there are issues with privacy on behalf of the consumer, the farmer, the rancher, whoever's using these products, but then also can we equate at these organizations? What's their accountability and can they hold the organization accountable for their implement? So that is a really quick run through. And there's some information about me and my company. Happy to take questions. Thank you very much. We have time for questions. Bruce? Can anyone ever be assured that if you have data with a company that if you want to delete it that it actually is deleted? Yeah. And a recent example is 23andMe and I've been hearing on the radio, oh, if you have data there you should go in and delete it. But if 23andMe was doing things correctly they've got multiple backups and is it, is data never, it will go on forever. Isn't that, that's my understanding? Yes. First I met the CPO of 23andMe 10 years ago. I was like, hey, never contributing. Yeah, me neither. But, so it's different. The United States as much as we had the foundational principles we are very behind on data protection and privacy regulations. Europe, South, America, Africa, Asia, much more aggressive with financial penalties. They have the right to deletion and there is a very specific process that has to be followed and they can have financial criminal penalties levy. In the United States you're starting to see some of that in different states. We don't have that overarching federal regulation. Interestingly enough what most product companies in the United States try to do is say if you want to have your account disabled your information will not be available anymore. Not the same as deletion but product and tech guys will try to use that language. With some of the emerging state right privacy regulations notably in California there are very specific data deletion requirements that and procedures that have to be followed that have those financial criminal penalties. But it's not nationwide yet. That's a great question. Okay, thank you. I'm just a question more of a comment on that person. I used to be a data protection officer for a media company. And I can tell you my job as a media as a BPO was not to protect your rights but to maximize the value of our data assets and to avoid fines and lawsuits. But I can tell you the development of truly delete data is really hard and takes a whole lot of work because you have to, as you said, you've got backups and backups for your backups. And something sitting on the developer's hard drive that he copied in under the development environment. Find all that. It's really hard. So you've got to do, so anyway, my expectation is that when you ask somebody to complete something that they're based in the United States and that's not going to happen. So, unless it's different related, then maybe. Well, and also I'll get to that. So I learned privacy from privacy engineering perspective. So I was not the typical lawyer into privacy. I was a practitioner who sat down with the back end engineers and coders and programmers and UI UX guys and figured out how to codify privacy into the technology stack as well as policy stack. So, it is much easier to engineer privacy controls and processes in the beginning phase of building because you have much less rework, cultural change that needs to happen, and lost files and databases. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah.